[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sr. Data Scientist at Feedzai\nMy interest lies at the intersection of software development and machine learning.\n\nRecent endeavors:\n\nGenerating baby names with Multilayered Perceptron\nSelf-attention with basic torch tensors\nGenerating shakespear with chatGPT like decoder"
  },
  {
    "objectID": "posts/2024-05-07-transformer-with-basic-torch/index.html",
    "href": "posts/2024-05-07-transformer-with-basic-torch/index.html",
    "title": "Building decoder only transformer similar to chatGPT with basic torch",
    "section": "",
    "text": "I spent some time coding up chatGPT using torch modules other than self-attention modules. The outcome was an auto-regressive (consumes its own output to produce more output) model trained on Shakespear’s writing. The output was by no means spectacular but wasn’t too bad for hacking at it couple of days and training for ~10 minutes. The comparison of the model’s output prior to training and after is quite a significant jump. The output before training is random garbage as expected but the output after training follows Shakespear like dialogues structure.\nLink to Code\nThis is a decoder only model so the fun part was coding the masked self-attention layer. In self attention all tokens are allowed to incorporate all tokens during creation of the context vector. Self attention is used in encoder only model where the end goal is to extract feature representation of the sequence which can be used downstream for other tasks like sentence classification. In masked self-attention on the other hand the model is not allowed to incorporate the tokens in the future. It can only look incorporate tokens in the past upto the context length specified during data creation. This is key because we want the model to generate new tokens based on the past tokens it has seen not the future.\nOutput of the model before training:\n\nsJgGNSW’Q!CmQx!MEcPl$.HjGA-G?CpK &tM.X$$oCaecJz-kmGKyEtkYAnx.p.EtXV.Ym$fE-zmksK;iuHECiZRaBXOSjPYj\n\nOutput after training for 10000 steps:\n\nsilter arturiarlys his blies.\nCORIOLANUS: It foe, The colse the warwech and apped! Would and heaven’d be! Efrerce’s toly neithe own is.\nPOLYCUS: Whom I have make usjulier’d?\nKING EFORY VON YORY: I what you hear somes, to might thine with ther half, hose secronior:\nESTERded liever men. Bene again sir, not’t As sweet blood with arm he at then hear, Or cravion sleep’d on own thou wouldstn! Hing in sorrow semkior propects, We this most: who fatthing this: You, meap to Motriss! and ears The will by arm her sales: brother! Angue with he weart him?\nMENENE’O: Now a counfectly grace, center-hour no plovous cht in with tyiots merch by unwas speak a, or stire, the hock from of sorrow fears.\n\nOfcourse the content of the text is totally non-sensical but we can see that the model has learned quite a lot compared to its untrained days. I have truncated the output for the sake of brevity here. The output follows writing style of an english sentence with (somewhat) appropriate use of punctuations. The output also contains inappropriate use of uppercase in the middle of the sentence.\nI did not push too hard to improve performance by tuning the hyperparameters. For this exercise I was interested in coding up the inner workings of a transformer based model to get a deeper understanding. I am satisfied with what I was able to get out of this exercise. It is quite interesting to see that the capacity of the architecture. It gives a sense of appreciation as to what it has achieved when trained at scale.\nI want to thank Andrej Karpathy for this excellent tutorial on the inner workings of chatGPT."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is place for my learnings, experiments and ideas that I have come accross that my future self will appreciate."
  },
  {
    "objectID": "posts/2025-05-14-compute-derivatives/gradient_descent.html",
    "href": "posts/2025-05-14-compute-derivatives/gradient_descent.html",
    "title": "Breaking down Back Propagation",
    "section": "",
    "text": "Background\nGradient descent is the optimization technique used to optimize the parameters of a system. I say system because gradient descent is a general purpose technique that can be used to optimize the parameters that influence any output. The assumption here is that the computations are differentiable. Back propagation lies at the heart of gradient descent. It is used to calculate the gradients (derivatives) of the parameters. The mechanical steps performed to compute the gradients of the parameters is back propagation. In this article we will see simple examples of how back propagation computes the gradients.\n\nSimple Neuron\nWe will be looking at Simple Neural Networks. Neural networks take in one or multidimensional input to produce one or multidimensional output. To do that they are composed of a bunch of neurons that are parallel to each other forming a layer. Multiple layer can be chained together to create “deep” neural networks.\n\n\n\n\n\n\nFigure 1: Simple Neuron\n\n\n\n\n\nForward Pass\nFigure 1 illustrates a simple neuron. It takes in an input, performs some computation with its weight and produces an output. Application of a non-linear activation function is not explicitly shown but can be thought of as part of the computation. Non-linear activation functions play a crucial role in giving Neural Networks the flexibility to solve non-linear problems. I have omitted the bias term for simplicity which can be thought of as the offset that gets added on before the activation function. The input could be multidimensional in which case the weight would also be multidimensional. The weights multiply the inputs and the results get summed up (the sum would apply to multiple weight input pairs and the bias term). The summed value is passed through the non-linear activation function producing the final output.\n\n\nDerivatives\nWe can clearly see that since we are using weight to multiply the input it affects the output. How much does the weight influence the output can be calculated by taking the derivate of the output with respect to the weight. Multidimensional derivatives are called gradients. This article uses derivatives and gradients interchangeably. If there are multiple weights (as is the case when the inputs are multidimensional), since each weight contributes (by multiplying) to the output, each weight will have a derivative with respect to the output. The derivative is the degree of influence the computation has in producing the output. When we want to influence the output in a certain direction we can use the derivative to adjust the weight.\n\n\nBackward pass\nAt a high level inputs are fed into the network which produces output. We can think of the outputs as the predictions of the model. In supervised learning we know the desired output of the each input. We can compare the output of the model and desired outputs (labels) to create a error value. We use a loss function for that. If we aggregate all the errors we get a scalar value which quantifies the mistakes the model made in the predictions. The wights played a role in creating the outputs thus the error. We can propagate the information captured int the error back to the weights giving the weights an opportunity to update in the desired direction of the output. The derivatives of the weights provide the direction in which the weights are to be adjusted.\n\n\n\n\n\n\nFigure 2: Chain rule\n\n\n\nAs there can be arbitrary number of neurons in a layer and arbitrary number of layers we need a systematic way to calculate the gridents of each weight. Back propagation is this systematic method that computes the gradients of the error with respect to each weight. In a deep neural network the output of previous layer are fed as input to the subsequent layer till the final error is computed. Since output of a computation is being fed as input into another computation which is a compositie function. Chain rule in calculus gives us a way to calculate the gradients for composite functions. The chain rule in calculus states that if you have a composite function, the derivative of the output with respect to an input variable is the product of the derivatives of the intermediate steps.\n\n\nChain rule\nFigure 2 illustrates forward and backward pass of a neural network that has only two neurons in two layers. In the forward pass the input (x) is influenced by w1 to produce x’. x’ in turn is influenced by w2 to produce output (o). Calculating the derivative of w2 with respect to o can be done in a single step. Derivative of w1 with respect to o needs to take into consideration the intermediate values computed along the way. The right thing to do here is to multiply the intermediate derivatives.\n\n\nConclusion\nThat is pretty much all there is to back propogation. The opertation to compute individual derivatives can be straight forward. The complexity rises from having to calculate and keep track of intermediate derivatives. Imagine hundreds of these layers and their associated computations. Frameworks like pytorch make is incredibly simple by abstracting away the complexity. Manually computing the gradients layer by layer would surely be tedious not to mention error prone. In this example we only looked at scalar valued output. In practical terms inputs are vectors and layers of neurons then turn into matrices. Vectors stacked on top of eachother become matrices so the computation end up becoming matrix matrix operations. Frameworks like pytorch also take this into consideration making gradient computation a simple .backward() method call.\n\n\nPostscript\nAndrej Karpathy has a video on creating micrograd; it is a simple codebase that has a simple class that does a bunch of operations. Each operation also has an associated method that accumulates the derivative. I would highly recommend watching the video. Below are some snippets of code on my attempt at implementing the class. It only shows the add method for brevity. When two Values are added the result output could further go on to participate in other computations. Instead of worrying about the global operation we only worry about the local derivative (which is 1 in our example of addition) multiplied with whatever the derivative of the output is with respect to the global output. As long as we systematically follow this starting from the end (back to front - crucial to propagate gradients correctly) we will have the correct derivaties.\n\nclass Value:\n    \"\"\"Class that implements basic autograd functionality similar to pytorch\"\"\"\n\n    def __init__(self, data, child=(), label=\"\", op=\"\"):\n        self.data = data\n        self._prev = set(child)\n        self.label = label\n        self._op = op\n        self.grad = 0 # Initially gradient is zero\n        self._backward = lambda : None\n\n    def __add__(self, other):\n        if isinstance(other, (int, float)): other = Value(other)\n        out = Value(self.data + other.data, child=(self, other), op=\"+\")\n        def backward():\n            self.grad += out.grad # We are omitting multiplying with local derivative which is * 1 in addition\n            other.grad += out.grad\n        out._backward = backward\n        return out\n\n\nResources:\n\nMy implementation of micrograd\nAndrej’s micrograd repo\nDrawing made possible by Excalidraw"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collection",
    "section": "",
    "text": "Breaking down Back Propagation\n\n\n\ndeep learning\n\ncode\n\nderivatives\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nishan dahal\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding decoder only transformer similar to chatGPT with basic torch\n\n\n\ndeep learning\n\ncode\n\nauto-regressive\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nishan dahal\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 4, 2024\n\n\nIshan Dahal\n\n\n\n\n\nNo matching items"
  }
]