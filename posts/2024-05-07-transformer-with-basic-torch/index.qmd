---
title: "Building decoder only transformer similar to chatGPT with basic torch"
author: "ishan dahal"
date: "2024-05-07"
categories: [deep learning, code, auto-regressive]
image: "image.jpg"
---


I spent some time coding up chatGPT using torch modules other than self-attention modules. The outcome was an auto-regressive (consumes its own output to produce more output) model trained on Shakespear's writing. The output was by no means spectacular but wasn't too bad for hacking at it couple of days and training for ~10 minutes. The comparison of the model's output prior to training and after is quite a significant jump. The output before training is random garbage as expected but the output after training follows Shakespear like dialogues structure.

<a href="https://github.com/ishandahal/transformer-playground/blob/main/src/train_chatGPT.py" target="_blank">Link to Code</a>

This is a decoder only model so the fun part was coding the masked self-attention layer. In self attention all tokens are allowed to incorporate all tokens during creation of the context vector. Self attention is used in encoder only model where the end goal is to extract feature representation of the sequence which can be used downstream for other tasks like sentence classification. In masked self-attention on the other hand the model is not allowed to incorporate the tokens in the future. It can only look incorporate tokens in the past upto the context length specified during data creation. This is key because we want the model to generate new tokens based on the past tokens it has seen not the future. 

__Output of the model before training:__

>sJgGNSW'Q!CmQx!MEcPl\$.HjGA-G?CpK &tM.X\$\$oCaecJz-kmGKyEtkYAnx.p.EtXV.Ym$fE-zmksK;iuHECiZRaBXOSjPYj  

__Output after training for 10000 steps:__

>
>silter arturiarlys his blies.
>
>CORIOLANUS:
>It foe,
>The colse the warwech and apped!
>Would and heaven'd be! Efrerce's toly neithe own is.
>
>POLYCUS:
>Whom I have make usjulier'd?
>
>KING EFORY VON YORY:
>I what you hear somes, to might thine with ther half, hose secronior:
>
>ESTERded liever men.
>Bene again sir, not't
>As sweet blood with arm he at then hear,
>Or cravion sleep'd on own thou wouldstn!
>Hing in sorrow semkior propects,
>We this most: who fatthing this:
>You, meap to Motriss! and ears
>The will by arm her sales: brother! Angue with he weart him?
>
>MENENE'O:
>Now a counfectly grace, center-hour no plovous
>cht in with tyiots merch by unwas speak a,
>or stire, the hock from of sorrow fears.

Ofcourse the content of the text is totally non-sensical but we can see that the model has learned quite a lot compared to its untrained days. I have truncated the output for the sake of brevity here. The output follows writing style of an english sentence with (somewhat) appropriate use of punctuations. The output also contains inappropriate use of uppercase in the middle of the sentence. 

I did not push too hard to improve performance by tuning the hyperparameters. For this exercise I was interested in coding up the inner workings of a transformer based model to get a deeper understanding. I am satisfied with what I was able to get out of this exercise. It is quite interesting to see that the capacity of the architecture. It gives a sense of appreciation as to what it has achieved when trained at scale.  

I want to thank Andrej Karpathy for <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=14" target="_blank">this</a> excellent tutorial on the inner workings of chatGPT. 





